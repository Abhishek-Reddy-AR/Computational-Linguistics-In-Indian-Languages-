{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5fcc9f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing req modules\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "99342aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the similarity dataset\n",
    "l=[]\n",
    "with open('hindi.txt','r',encoding=\"utf8\") as fp:\n",
    "    l.extend(fp.readlines())\n",
    "#getting the similarity dataset pairs and ground truths\n",
    "pairs=[]\n",
    "for i in range(len(l)-1):\n",
    "    pairs.append(l[i].split(','))\n",
    "#storing different thresholds\n",
    "thresholds=[0.4,0.5,0.6,0.7,0.8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "064afe6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to load the glove embeddings\n",
    "def load_glove_embeddings(File):\n",
    "    #to store embeddings\n",
    "    glove_embeddings = {}\n",
    "    #opening glove file\n",
    "    with open(File,'rb') as f:\n",
    "        #for every line, every line consists word first and its  100d embedding vec next\n",
    "        for line in f:\n",
    "            #splitting\n",
    "            split_line = line.split()\n",
    "            #getting the word\n",
    "            word = split_line[0]\n",
    "            #getting the words glove embedding\n",
    "            embedding = np.array(split_line[1:], dtype=np.float64)\n",
    "            #adding word and its embedding to dict\n",
    "            glove_embeddings[word] = embedding\n",
    "    #returning the embeddings\n",
    "    return glove_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ba798d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to compute cosine similarity when give two vectors of same dim\n",
    "def cos_sim(v1,v2):\n",
    "    #getting the denominarors\n",
    "    a=math.sqrt(sum([x*x for x in v1]))\n",
    "    b=math.sqrt(sum([x*x for x in v2]))\n",
    "    #getting the numerator\n",
    "    num=0\n",
    "    for i in range(len(v1)):\n",
    "        num+=(v1[i]*v2[i])\n",
    "    #returning cos sim value\n",
    "    return num/(a*b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f9ec99db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to calculate accuracy when given the embedding model,embedding data,threshold as parameters\n",
    "def cal_acc(m_type,data,threshold,dimension):\n",
    "    #when the model type is either continous bag of words or skipgrams or fasttext\n",
    "    #the data is list consisting of numpy array of  word vectors  and indexes dict which map word to the corresponding index in the numpy array \n",
    "    if m_type in ['cbow','sg','ft']:\n",
    "        #getting word to index mapping\n",
    "        indexes_dict=data[0]\n",
    "        #getting word vectors numpy array\n",
    "        word_vects=data[1]\n",
    "        #to store accuracy\n",
    "        acc=0\n",
    "        #used to write to csv file\n",
    "        df=pd.DataFrame(columns=['word1','word2','similarity_score','ground_truth_similarity_score','label'])\n",
    "        #for each pair\n",
    "        for i,j,k in pairs:\n",
    "            #getting embedding vec's for the pair\n",
    "            v1=word_vects[indexes_dict[i]]\n",
    "            v2=word_vects[indexes_dict[j]]\n",
    "            #calculating the similarity score\n",
    "            sim=cos_sim(v1,v2)\n",
    "            #creating a dict for current pair\n",
    "            d={'word1':i,'word2':j,'similarity_score':sim*10,'ground_truth_similarity_score':float(k[:-1]),'label':0}\n",
    "            #if similarity score is greater than threshold\n",
    "            if (sim>=threshold and float(k[:-1])>=threshold*10)or(sim<threshold and float(k[:-1])<threshold*10):\n",
    "                #increasing acc by 1\n",
    "                acc+=1\n",
    "                #assining label as 1\n",
    "                d['label']=1\n",
    "            #adding to dataframe\n",
    "            df=df.append(d, ignore_index = True)\n",
    "        #adding the accuracy score to dataframe\n",
    "        d={'word1':'accuracy is:'+str(acc/len(pairs))}\n",
    "        df=df.append(d, ignore_index = True)\n",
    "        #writing to csv file\n",
    "        file_name='Q1_'+m_type+'_similarity_'+str(int(threshold*10))+'_'+dimension+'.csv'\n",
    "        df.to_csv(file_name, index=False)\n",
    "\n",
    "    #else if the embedding type is glove,then the data is a dict with word as key and its embedding as value\n",
    "    elif m_type=='glove':\n",
    "        #to store accuracy\n",
    "        acc=0\n",
    "        #used to write to csv file\n",
    "        df=pd.DataFrame(columns=['word1','word2','similarity_score','ground_truth_similarity_score','label'])\n",
    "        #for each pair\n",
    "        for i,j,k in pairs:\n",
    "            #getting embedding vec's for the pair\n",
    "            #in the dict the words are stored as encoded values\n",
    "            v1=data[i.encode()]\n",
    "            v2=data[j.encode()]\n",
    "            #calculating the similarity score\n",
    "            sim=cos_sim(v1,v2)\n",
    "            #creating a dict for current pair\n",
    "            d={'word1':i,'word2':j,'similarity_score':sim*10,'ground_truth_similarity_score':float(k[:-1]),'label':0}\n",
    "            #if similarity score is greater than threshold\n",
    "            if (sim>=threshold and float(k[:-1])>=threshold*10)or(sim<threshold and float(k[:-1])<threshold*10):\n",
    "                #increasing acc by 1\n",
    "                acc+=1\n",
    "                #assining label as 1\n",
    "                d['label']=1\n",
    "            #adding to dataframe\n",
    "            df=df.append(d, ignore_index = True)\n",
    "        #adding the accuracy score to dataframe\n",
    "        d={'word1':'accuracy is:'+str(acc/len(pairs))}\n",
    "        df=df.append(d, ignore_index = True)\n",
    "        #writing to csv file\n",
    "        file_name='Q1_'+m_type+'_similarity_'+str(int(threshold*10))+'_'+dimension+'.csv'\n",
    "        df.to_csv(file_name, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ccebb6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading cbow model\n",
    "model = Word2Vec.load('hi-d100-m2-cbow.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9ac065a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the cbow word embeddings array with 100 dimensions\n",
    "n=np.load('hi-d100-m2-cbow.model.wv.vectors.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ffe7c664",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting vocab\n",
    "vocab=list(model.wv.vocab)\n",
    "#dict to store the word to its corresponding index in the numpy array of embedding vects\n",
    "ind={}\n",
    "for key in vocab:\n",
    "    ind[key]=-1\n",
    "#getting the word and its corresponding index in numpy array\n",
    "dict_wv={}\n",
    "for key in vocab:\n",
    "    dict_wv[tuple(model.wv.word_vec(key))]=key\n",
    "for i in range(len(n)):\n",
    "    cr=dict_wv[tuple(n[i])]\n",
    "    ind[cr]=i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9ba6d39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#storing the word to index mapping and numpy vectors of embeddings\n",
    "data=[ind,n]\n",
    "#performing word similarity using cbow embeddings\n",
    "for threshold in thresholds:\n",
    "    cal_acc('cbow',data,threshold,'100d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c8483e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the cbow word embeddings array with 50 dimensions\n",
    "n=np.load('hi-d50-m2-cbow.model.wv.vectors.npy')\n",
    "#storing the word to index mapping and numpy vectors of embeddings\n",
    "data=[ind,n]\n",
    "#performing word similarity using cbow embeddings\n",
    "for threshold in thresholds:\n",
    "    cal_acc('cbow',data,threshold,'50d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "855764c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading skipgram embeddings 100 dimensional\n",
    "s = np.load('hi-d100-m2-sg.model.wv.vectors.npy')\n",
    "data=[ind,s]\n",
    "#performing word similarity using skipgrams embeddings\n",
    "for threshold in thresholds:\n",
    "    cal_acc('sg',data,threshold,'100d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "65167254",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading skipgram embeddings 50 dimensional\n",
    "s = np.load('hi-d50-m2-sg.model.wv.vectors.npy')\n",
    "data=[ind,s]\n",
    "#performing word similarity using skipgrams embeddings\n",
    "for threshold in thresholds:\n",
    "    cal_acc('sg',data,threshold,'50d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d2a2f9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading fasttext embeddings 100 dimensional\n",
    "f=np.load('hi-d100-m2-fasttext.model.wv.vectors.npy')\n",
    "data=[ind,f]\n",
    "#performing word similarity using fasttext embeddigs\n",
    "for threshold in thresholds:\n",
    "    cal_acc('ft',data,threshold,'100d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f3bdb6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading fasttext embeddings 50 dimensional\n",
    "f=np.load('hi-d50-m2-fasttext.model.wv.vectors.npy')\n",
    "data=[ind,f]\n",
    "#performing word similarity using fasttext embeddigs\n",
    "for threshold in thresholds:\n",
    "    cal_acc('ft',data,threshold,'50d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c454af49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#loading glove embeddings 100 dimensional\n",
    "g=load_glove_embeddings('hi-d100-glove.txt')\n",
    "#performing word similarity using glove embeddigs\n",
    "for threshold in thresholds:\n",
    "    cal_acc('glove',g,threshold,'100d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7bd98db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading glove embeddings 50 dimensional\n",
    "g=load_glove_embeddings('hi-d50-glove.txt')\n",
    "#performing word similarity using glove embeddigs\n",
    "for threshold in thresholds:\n",
    "    cal_acc('glove',g,threshold,'50d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71276f32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcbd373",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
