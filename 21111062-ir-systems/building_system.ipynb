{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "789b866a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing required module\n",
    "\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from unidecode import unidecode\n",
    "from os.path import join\n",
    "import math\n",
    "import re\n",
    "import os\n",
    "import pickle\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b438355c",
   "metadata": {},
   "source": [
    "### Q1.Functions for Processing of the Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7ac88a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to convert document into list of tokens\n",
    "def Tokenizer(text):\n",
    "    \n",
    "    #tokenizing based on whitespace character\n",
    "    tk = WhitespaceTokenizer()\n",
    "    text=tk.tokenize(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93202bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to remove stopwords from the token list\n",
    "def RemoveStopWords(text_list):\n",
    "    \n",
    "    #getting the stopwords\n",
    "    stop = stopwords.words('english')\n",
    "    \n",
    "    #to store tokens that are not stopwords\n",
    "    new_list=[]\n",
    "    \n",
    "    #for each token in the list\n",
    "    for i in text_list:\n",
    "        \n",
    "        #if the token is not a stopword adding it to new list\n",
    "        if i not in stop:\n",
    "            new_list.append(i)\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73af2bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that does stemming on the token list\n",
    "def Stemmer(text_list):\n",
    "    \n",
    "    #to store stemmed tokens\n",
    "    stem_list=[]\n",
    "    \n",
    "    #we use porterstemmer\n",
    "    ps = PorterStemmer()\n",
    "    \n",
    "    #for each token\n",
    "    for i in text_list:\n",
    "        \n",
    "        #adding stemmed token to the stem list\n",
    "        stem_list.append(ps.stem(i))\n",
    "    return stem_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f4377eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    \n",
    "    #replacing non ascii characters \n",
    "    text=unidecode(text)\n",
    "    #converting to lowercase\n",
    "    text= text.lower()\n",
    "    \n",
    "    #removing the html tags\n",
    "    clean = re.compile('<.*?>') \n",
    "    text= re.sub(clean, '', text)\n",
    "    \n",
    "    #replacing '\\n' ,'\\r' and punctuations with ' '(space)\n",
    "    text = text.replace(\"\\n\",\" \").replace(\"\\r\",\" \")\n",
    "    text = text.replace(\"'s\",\" \")\n",
    "    punctuationList = '!\"#$%&\\()*+,-./:;<=>?@[\\\\]^_{|}~'\n",
    "    x = str.maketrans(dict.fromkeys(punctuationList,\" \"))\n",
    "    text = text.translate(x)\n",
    "    \n",
    "    #performing tokenization,stopword removal and stemming\n",
    "    token_list=Tokenizer(text)\n",
    "    token_list=RemoveStopWords(token_list)\n",
    "    token_list=Stemmer(token_list)\n",
    "    \n",
    "    return token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cc50b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the current working directory\n",
    "cwd = os.getcwd() \n",
    "#storing all the documents names present in the corpus\n",
    "files = os.listdir('english-corpora/') \n",
    "\n",
    "#list to store the contents of each document\n",
    "corpus=[]\n",
    "#for each document\n",
    "for i in range(len(files)):\n",
    "    #opening the doc\n",
    "    #fileopen = open('english-corpora\\\\'+files[i], 'r',encoding=\"utf8\")\n",
    "    fileopen = io.open(join('english-corpora/',str(files[i])),'r',encoding='utf-8',errors='ignore')\n",
    "    #reading and appending the contents of doc into text list\n",
    "    source = fileopen.read()\n",
    "    corpus.append(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e0048da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing the .txt extention and storing only the doc ids\n",
    "#sfiles=[x[:-4] for x in files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a303204",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to store the token list of each doc \n",
    "processed_corpus=[]\n",
    "#for each doc\n",
    "for x,i in enumerate(corpus):\n",
    "    #appending the token list returned by the preprocess function\n",
    "    processed_corpus.append(preprocess(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1892ed",
   "metadata": {},
   "source": [
    "### Q2. Building IR Systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e00ce7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to calculate term freq,document freq,doc length,corpus size \n",
    "def info(processed_corpus):\n",
    "    #list to store term freq of each token in each doc in the corpus\n",
    "    tf = []\n",
    "    #dict to store the doc freq of each token in the corpus and also the list of docids that contain that token\n",
    "    df = {}\n",
    "    #list to store the len of each doc\n",
    "    doc_len = []\n",
    "    #to store the tot no of docs\n",
    "    corpus_size = 0\n",
    "    \n",
    "    #for each doc in the processed corpus\n",
    "    for document in processed_corpus:\n",
    "        #increasing the corpus size by 1\n",
    "        corpus_size += 1\n",
    "        #storing the len of the doc \n",
    "        doc_len.append(len(document))\n",
    "\n",
    "        #computing tf (term frequency) per document\n",
    "        frequencies = {}\n",
    "        for term in document:\n",
    "            term_count = frequencies.get(term, 0) + 1\n",
    "            frequencies[term] = term_count\n",
    "        #appending the frequencies dict of that doc into tf list\n",
    "        tf.append(frequencies)\n",
    "\n",
    "        #computing df (document frequency) per token\n",
    "        for term, _ in frequencies.items():\n",
    "            if term in df.keys():\n",
    "                df[term]['count']+=1\n",
    "                df[term]['l'].append(corpus_size-1)\n",
    "            else:\n",
    "                df[term]={'count':1,'l':[corpus_size-1]}\n",
    "    \n",
    "    return tf,df,doc_len,corpus_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d16b60aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calling info function on processed corpus\n",
    "tf,df,doc_len,corpus_size=info(processed_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55953255",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9befd228",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to calculate inverse doc freq of each token\n",
    "def cal_idf(df,corpus_size):\n",
    "    #dict to store idf \n",
    "    idf={}\n",
    "    #calculating idf of each token and storing it in dict with key as token and value as idf of that token\n",
    "    for token,value in df.items():\n",
    "        idf[token]=math.log(corpus_size/(value['count']+1))\n",
    "    return idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "47294cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to calculate tf idf value of each token in each doc\n",
    "def cal_tf_idf(tf,idf):\n",
    "    #list to store tf idf values of each token in each doc\n",
    "    tf_idf=[]\n",
    "    \n",
    "    #for term freq dict of each doc\n",
    "    for i in tf:\n",
    "        #to store the tf idf values of the tokens in curr doc\n",
    "        curr_dict={}\n",
    "        #getting the total no of words in the curr doc\n",
    "        no_of_words=sum(i.values())\n",
    "        #for each token in the term freq dict of curr doc\n",
    "        for token in i.keys():\n",
    "            #calculating tf idf of that token in the curr doc \n",
    "            curr_dict[token]=(i[token]/no_of_words)*idf[token]\n",
    "        #appending the tf idf dict of curr doc into tf idf list\n",
    "        tf_idf.append(curr_dict)\n",
    "    return tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "98574726",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating idf and tf idf\n",
    "idf=cal_idf(df,corpus_size)\n",
    "tf_idf=cal_tf_idf(tf,idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b878acec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "668e59d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to calculate the idf of bm25\n",
    "def cal_idf_bm25(df,corpus_size):\n",
    "    #dict to store idf of bm25\n",
    "    idf_bm25={}\n",
    "    #calculating bm25 idf of each token and storing it in dict with key as token and value as bm25 idf of that token\n",
    "    for token, value in df.items():\n",
    "            idf_bm25[token] = math.log(1 + (corpus_size - value['count'] + 0.5) / (value['count'] + 0.5))\n",
    "    return idf_bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ab531da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting bm25 idf\n",
    "idf_bm25=cal_idf_bm25(df,corpus_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816d8c4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "65081039",
   "metadata": {},
   "outputs": [],
   "source": [
    "#storing all the calculated variables as pickle files\n",
    "\n",
    "tf_file= open('stored_tf', \"wb\")\n",
    "pickle.dump(tf, tf_file)\n",
    "tf_file.close()\n",
    "\n",
    "df_file= open('stored_df', \"wb\")\n",
    "pickle.dump(df, df_file)\n",
    "df_file.close()\n",
    "\n",
    "idf_file= open('stored_idf', \"wb\")\n",
    "pickle.dump(idf, idf_file)\n",
    "idf_file.close()\n",
    "\n",
    "tf_idf_file= open('stored_tf_idf', \"wb\")\n",
    "pickle.dump(tf_idf, tf_idf_file)\n",
    "tf_idf_file.close()\n",
    "\n",
    "idf_bm25_file= open('stored_idf_bm25', \"wb\")\n",
    "pickle.dump(idf_bm25, idf_bm25_file)\n",
    "idf_bm25_file.close()\n",
    "\n",
    "doc_len_file=open('stored_doc_len',\"wb\")\n",
    "pickle.dump(doc_len,doc_len_file)\n",
    "doc_len_file.close()\n",
    "\n",
    "file_names_file=open('file_names',\"wb\")\n",
    "pickle.dump(files,file_names_file)\n",
    "file_names_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39eb83c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
