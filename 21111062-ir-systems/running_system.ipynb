{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a85458f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing required module\n",
    "\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from unidecode import unidecode\n",
    "import math\n",
    "import re\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sys import argv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6f4f710",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the stored data required for running the three systems\n",
    "\n",
    "tf= pickle.load(open('stored_tf', \"rb\"))\n",
    "df= pickle.load(open('stored_df', \"rb\"))\n",
    "idf= pickle.load(open('stored_idf', \"rb\"))\n",
    "tf_idf= pickle.load(open('stored_tf_idf', \"rb\"))\n",
    "idf_bm25= pickle.load(open('stored_idf_bm25', \"rb\"))\n",
    "doc_len=pickle.load(open('stored_doc_len',\"rb\"))\n",
    "files=pickle.load(open('file_names',\"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692fe3e7",
   "metadata": {},
   "source": [
    "### We perform preprocessing of the query in the same way we have done for a document in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4f33a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to convert query into list of tokens\n",
    "def Tokenizer(query):\n",
    "    \n",
    "    #tokenizing based on whitespace character\n",
    "    tk = WhitespaceTokenizer()\n",
    "    query_tokens=tk.tokenize(query)\n",
    "    return query_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "158cbbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to remove stopwords from query tokens\n",
    "def RemoveStopWords(query_tokens):\n",
    "    \n",
    "    #getting the stopwords\n",
    "    stop = stopwords.words('english')\n",
    "    \n",
    "    #to store tokens that are not stopwords\n",
    "    new_list=[]\n",
    "    \n",
    "    #for each token in the list\n",
    "    for i in query_tokens:\n",
    "        \n",
    "        #if the token is not a stopword adding it to new list\n",
    "        if i not in stop:\n",
    "            new_list.append(i)\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ae2eadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that does stemming on the query tokens\n",
    "def Stemmer(query_tokens):\n",
    "    \n",
    "    #to store stemmed tokens\n",
    "    stem_list=[]\n",
    "    \n",
    "    #we use porterstemmer\n",
    "    ps = PorterStemmer()\n",
    "    \n",
    "    #for each token\n",
    "    for i in query_tokens:\n",
    "        \n",
    "        #adding stemmed token to the stem list\n",
    "        stem_list.append(ps.stem(i))\n",
    "    return stem_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2a92f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(query):\n",
    "    \n",
    "    #replacing non ascii characters \n",
    "    query=unidecode(query)\n",
    "    #converting to lowercase\n",
    "    query= query.lower()\n",
    "    \n",
    "    #removing the html tags\n",
    "    clean = re.compile('<.*?>') \n",
    "    query= re.sub(clean, '', query)\n",
    "    \n",
    "    #replacing '\\n' ,'\\r' and punctuations with ' '(space)\n",
    "    query = query.replace(\"\\n\",\" \").replace(\"\\r\",\" \")\n",
    "    query = query.replace(\"'s\",\" \")\n",
    "    punctuationList = '!\"#$%&\\()*+,-./:;<=>?@[\\\\]^_{|}~'\n",
    "    x = str.maketrans(dict.fromkeys(punctuationList,\" \"))\n",
    "    query = query.translate(x)\n",
    "    \n",
    "    #performing tokenization,stopword removal and stemming\n",
    "    query_tokens=Tokenizer(query)\n",
    "    query_tokens=RemoveStopWords(query_tokens)\n",
    "    query_tokens=Stemmer(query_tokens)\n",
    "    \n",
    "    return query_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48c1220",
   "metadata": {},
   "source": [
    "### Q2. Running IR Systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "753048ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to calculate the boolean score \n",
    "# args: query_tokens , documnent frequencies, top_k:no of top documents to be retrieved\n",
    "\n",
    "def boolean_score(query_tokens,df,top_k):\n",
    "    \n",
    "    #to store the retrieved doc ids\n",
    "    docs=[]\n",
    "    \n",
    "    #for each token in the query\n",
    "    for token in query_tokens:\n",
    "        #if the token is present in df\n",
    "        if token in df.keys():\n",
    "            #we add the doc ids that contain that token into the list\n",
    "            docs.extend(df[token]['l'])\n",
    "    \n",
    "    #getting the unique docids in the list\n",
    "    s=set(docs)\n",
    "    #to store the count of each docid in the docs list\n",
    "    final_count=[]\n",
    "    #for each docid\n",
    "    for i in s:\n",
    "        #appending its count i.e the no of query tokens present in that partiular docid\n",
    "        final_count.append([docs.count(i),files[i]])\n",
    "    #sorting in dec order based on count values\n",
    "    final_count.sort(reverse=True)\n",
    "    #to store only the top k docids\n",
    "    doc_ids=[]\n",
    "    for i in range(top_k):\n",
    "        doc_ids.append(final_count[i][1])\n",
    "    return doc_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2f0f05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to calculate td idf score\n",
    "#args :  query_tokens , tf_idf ,idf ,top_k:no of top documents to be retrieved\n",
    "\n",
    "def tf_idf_score(query_tokens,tf_idf,idf,top_k):\n",
    "    #to store cosine similarity scores \n",
    "    cos_scores=[]\n",
    "    #getting the normalized term freq of tokens in the query\n",
    "    tf_q={}\n",
    "    for i in set(query_tokens):\n",
    "        tf_q[i]=query_tokens.count(i)/len(query_tokens)\n",
    "    \n",
    "    #getting the tf idf values for the tokens in the query\n",
    "    tf_idf_q={}\n",
    "    for i in tf_q.keys():\n",
    "        if i in idf.keys():\n",
    "            tf_idf_q[i]=tf_q[i]*idf[i]\n",
    "    \n",
    "    #getting the euclidean norm of query tokens  tf_idf values \n",
    "    a=math.sqrt(sum([x*x for x in tf_idf_q.values()]))\n",
    "    \n",
    "    #for each doc \n",
    "    for i in range(len(tf_idf)):\n",
    "        #if there are no tokens in that doc \n",
    "        if(len(tf_idf[i])==0):\n",
    "            #then ist cosine sim score is zero\n",
    "            cos_scores.append([0,files[i]])\n",
    "            continue\n",
    "        #else if there are tokens in the doc\n",
    "        curr_score=float(0.0)\n",
    "        \n",
    "        #for each token in the query\n",
    "        for j in tf_idf_q.keys():\n",
    "            #if that token is present in that doc\n",
    "            if j in tf_idf[i].keys():\n",
    "                #multiplying the tf idf values of that token (tf_idf of corpus * tf_idf of query)\n",
    "                curr_score+=tf_idf[i][j]*tf_idf_q[j]\n",
    "        \n",
    "        #getting the euclidean norm of doc tokens  tf_idf values \n",
    "        b=math.sqrt(sum([x*x for x in tf_idf[i].values()]))\n",
    "        \n",
    "        #calculating the current documents cosine similarity score\n",
    "        curr_score=(curr_score)/(a*b)\n",
    "        \n",
    "        #appending the docid and the score\n",
    "        cos_scores.append([curr_score,files[i]])\n",
    "    \n",
    "    #sorting in dec order based on the cosine similarity scores\n",
    "    cos_scores.sort(reverse=True)\n",
    "    #returning the top k docids\n",
    "    doc_ids=[]\n",
    "    for i in range(top_k):\n",
    "        doc_ids.append(cos_scores[i][1])\n",
    "    return doc_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a50f7e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to calculate the bm25 score\n",
    "#args  :  query_tokens , tf ,idf_bm25 , doc lengths , top_k:no of top documents to be retrieved ,k1 (hyperparameter), b(hyperparameter)\n",
    "\n",
    "def bm25_score(query_tokens,tf,idf_bm25,doc_len,top_k,k1,b):\n",
    "    \n",
    "    #calculating the avg doc len\n",
    "    avg_doc_len = sum(doc_len) / len(doc_len)\n",
    "    \n",
    "    #to store scores\n",
    "    bm25_scores=[]\n",
    "    #for each doc\n",
    "    for i in range(len(tf)):\n",
    "        #getting curr docs term freqs dict and its length\n",
    "        curr_score=0.0\n",
    "        curr_doc_len = doc_len[i]\n",
    "        frequencies = tf[i]\n",
    "        \n",
    "        #for each query token\n",
    "        for token in query_tokens:\n",
    "            #if token not in the curr doc , skip\n",
    "            if token not in frequencies:\n",
    "                continue\n",
    "            #if the token is present in the curr doc\n",
    "            freq = frequencies[token]\n",
    "            \n",
    "            #calculating bm25 score of that token and adding it to bm25 score of the curr doc\n",
    "            numerator = idf_bm25[token] * freq * (k1 + 1)\n",
    "            denominator = freq + k1 * (1 - b + b * curr_doc_len / avg_doc_len)\n",
    "            curr_score += (numerator / denominator)\n",
    "        #appending the docid and the bm25 score of that doc\n",
    "        bm25_scores.append([curr_score,files[i]])\n",
    "    #sorting the list based on bm25 scores in dec order\n",
    "    bm25_scores.sort(reverse=True)\n",
    "    #returning the top k docids\n",
    "    doc_ids=[]\n",
    "    for i in range(top_k):\n",
    "        doc_ids.append(bm25_scores[i][1])\n",
    "    return doc_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d077bb",
   "metadata": {},
   "source": [
    "### Q3. Reading Queries and storing the output from the systems in QRels format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b79f8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the queries\n",
    "query_file = argv[1]\n",
    "query_f = open(query_file, 'r')\n",
    "queries = query_f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8ddf8767",
   "metadata": {},
   "outputs": [],
   "source": [
    "#seperating the queryid and query\n",
    "queries_list=[]\n",
    "for query in queries:\n",
    "    queries_list.append(query.split(\"\\t\",1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fdfc8c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing last element of the list if it contains only '/n'\n",
    "if len(queries_list[-1])==1:\n",
    "    queries_list=queries_list[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "838b1d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating dataframes to store the qrels format results of each of the systems\n",
    "boolean_qrels=pd.DataFrame(columns=['QueryId', 'Iteration', 'DocId', 'Relevance'])\n",
    "tf_idf_qrels=pd.DataFrame(columns=['QueryId', 'Iteration', 'DocId', 'Relevance'])\n",
    "bm25_qrels=pd.DataFrame(columns=['QueryId', 'Iteration', 'DocId', 'Relevance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1906915c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for each query\n",
    "for query in queries_list:\n",
    "    #preprocessing the query\n",
    "    query_tokens=list(preprocess(query[1]))\n",
    "    \n",
    "    #getting the top 10 docs ids using boolen based, tf idf based , bm25 based retrieval systems\n",
    "    boolean_docids=boolean_score(query_tokens,df,5)\n",
    "    tf_idf_docids=tf_idf_score(query_tokens,tf_idf,idf,5)\n",
    "    bm25_docids=bm25_score(query_tokens,tf,idf_bm25,doc_len,5,1.5,0.75)\n",
    "    \n",
    "    #creating temporary dataframes to store the current query's retrieved docis\n",
    "    tempdf1=pd.DataFrame(columns=['QueryId', 'Iteration', 'DocId', 'Relevance'])\n",
    "    tempdf2=pd.DataFrame(columns=['QueryId', 'Iteration', 'DocId', 'Relevance'])\n",
    "    tempdf3=pd.DataFrame(columns=['QueryId', 'Iteration', 'DocId', 'Relevance'])\n",
    "    \n",
    "    #assings the docids\n",
    "    tempdf1.DocId=boolean_docids\n",
    "    tempdf2.DocId=tf_idf_docids\n",
    "    tempdf3.DocId=bm25_docids\n",
    "    \n",
    "    #assigns queryid , iteration,relevance values\n",
    "    tempdf1.QueryId=tempdf2.QueryId=tempdf3.QueryId=query[0]\n",
    "    tempdf1.Iteration=tempdf2.Iteration=tempdf3.Iteration=1\n",
    "    tempdf1.Relevance=tempdf2.Relevance=tempdf3.Relevance=1\n",
    "    \n",
    "    #appendig these temporary dataframes into their respective retrieval system dataframes\n",
    "    boolean_qrels=boolean_qrels.append(tempdf1)\n",
    "    tf_idf_qrels=tf_idf_qrels.append(tempdf2)\n",
    "    bm25_qrels=bm25_qrels.append(tempdf3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ea2d0153",
   "metadata": {},
   "outputs": [],
   "source": [
    "#storing the dataframes into csv files\n",
    "boolean_qrels.to_csv('boolean_qrels.csv',index=False,header=False)\n",
    "tf_idf_qrels.to_csv('tf_idf_qrels.csv',index=False,header=False)\n",
    "bm25_qrels.to_csv('bm25_qrels.csv',index=False,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4638405",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
